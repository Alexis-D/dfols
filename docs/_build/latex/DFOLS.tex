%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage[dontkeepoldnames]{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{DFO-LS Documentation}
\date{05 February 2018}
\release{1.0}
\author{Lindon Roberts}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}


\sphinxstylestrong{Release:} 1.0

\sphinxstylestrong{Date:} 05 February 2018

\sphinxstylestrong{Author:} \sphinxhref{mailto:lindon.roberts@maths.ox.ac.uk}{Lindon Roberts}

DFO-LS is a flexible package for finding local solutions to nonlinear least-squares minimization problems (with optional bound constraints), without requiring any derivatives of the objective. DFO-LS stands for Derivative-Free Optimizer for Least-Squares.

That is, DFO-LS solves
\begin{equation*}
\begin{split}\min_{x\in\mathbb{R}^n}  &\quad  f(x) := \sum_{i=1}^{m}r_{i}(x)^2 \\
\text{s.t.} &\quad  a \leq x \leq b\end{split}
\end{equation*}
Full details of the DFO-LS algorithm are given in our paper: C. Cartis, J. Fiala, B. Marteau and L. Roberts, Improving the Flexibility and Robustness of Model-Based Derivative-Free Optimization Solvers, technical report, University of Oxford, (2017).

DFO-LS is released under the GNU General Public License. Please \sphinxhref{http://www.nag.com/content/worldwide-contact-information}{contact NAG} for alternative licensing.


\chapter{Installing DFO-LS}
\label{\detokenize{install:dfo-ls-derivative-free-optimizer-for-least-squares-minimization}}\label{\detokenize{install::doc}}\label{\detokenize{install:installing-dfo-ls}}

\section{Requirements}
\label{\detokenize{install:requirements}}
DFO-LS requires the following software to be installed:
\begin{itemize}
\item {} 
Python 2.7 or Python 3 (\sphinxurl{http://www.python.org/})

\end{itemize}

Additionally, the following python packages should be installed (these will be installed automatically if using \sphinxstyleemphasis{pip}, see {\hyperref[\detokenize{install:installation-using-pip}]{\sphinxcrossref{Installation using pip}}}):
\begin{itemize}
\item {} 
NumPy 1.11 or higher (\sphinxurl{http://www.numpy.org/})

\item {} 
SciPy 0.18 or higher (\sphinxurl{http://www.scipy.org/})

\item {} 
Pandas 0.17 or higher (\sphinxurl{http://pandas.pydata.org/})

\end{itemize}


\section{Installation using pip}
\label{\detokenize{install:installation-using-pip}}
For easy installation, use \sphinxstyleemphasis{pip} (\sphinxurl{http://www.pip-installer.org/}) as root:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} [sudo] pip install dfols
\end{sphinxVerbatim}

or alternatively \sphinxstyleemphasis{easy\_install}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} [sudo] easy\PYGZus{}install dfols
\end{sphinxVerbatim}

If you do not have root privileges or you want to install DFO-LS for your private use, you can use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip install \PYGZhy{}\PYGZhy{}user dfols
\end{sphinxVerbatim}

which will install DFO-LS in your home directory.

Note that if an older install of DFO-LS is present on your system you can use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} [sudo] pip install \PYGZhy{}\PYGZhy{}upgrade dfols
\end{sphinxVerbatim}

to upgrade DFO-LS to the latest version.


\section{Manual installation}
\label{\detokenize{install:manual-installation}}
Alternatively, you can download the source code from \sphinxhref{https://github.com/numericalalgorithmsgroup/dfols}{Github} and unpack as follows:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} git clone https://github.com/numericalalgorithmsgroup/dfols
\PYGZdl{} \PYG{n+nb}{cd} dfols
\end{sphinxVerbatim}
\end{quote}

DFO-LS is written in pure Python and requires no compilation. It can be installed using:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} \PYG{o}{[}sudo\PYG{o}{]} pip install .
\end{sphinxVerbatim}
\end{quote}

If you do not have root privileges or you want to install DFO-LS for your private use, you can use:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip install \PYGZhy{}\PYGZhy{}user .
\end{sphinxVerbatim}
\end{quote}

instead.


\section{Testing}
\label{\detokenize{install:testing}}
If you installed DFO-LS manually, you can test your installation by running:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} python setup.py \PYG{n+nb}{test}
\end{sphinxVerbatim}
\end{quote}

Alternatively, the HTML documentation provides some simple examples of how to run DFO-LS.


\section{Uninstallation}
\label{\detokenize{install:uninstallation}}
If DFO-LS was installed using \sphinxstyleemphasis{pip} you can uninstall as follows:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} \PYG{o}{[}sudo\PYG{o}{]} pip uninstall dfols
\end{sphinxVerbatim}
\end{quote}

If DFO-LS was installed manually you have to remove the installed files by hand (located in your python site-packages directory).


\chapter{Overview}
\label{\detokenize{info::doc}}\label{\detokenize{info:overview}}

\section{When to use DFO-LS}
\label{\detokenize{info:when-to-use-dfo-ls}}
DFO-LS is designed to solve the nonlinear least-squares minimization problem (with optional bound constraints)
\begin{equation*}
\begin{split}\min_{x\in\mathbb{R}^n}  &\quad  f(x) := \sum_{i=1}^{m}r_{i}(x)^2 \\
\text{s.t.} &\quad  a \leq x \leq b\end{split}
\end{equation*}
We call \(f(x)\) the objective function and \(r_i(x)\) the residual functions (or simply residuals).

DFO-LS is a \sphinxstyleemphasis{derivative-free} optimization algorithm, which means it does not require the user to provide the derivatives of \(f(x)\) or \(r_i(x)\), nor does it attempt to estimate them internally (by using finite differencing, for instance).

There are two main situations when using a derivative-free algorithm (such as DFO-LS) is preferable to a derivative-based algorithm (which is the vast majority of least-squares solvers).

If \sphinxstylestrong{the residuals are noisy}, then calculating or even estimating their derivatives may be impossible (or at least very inaccurate). By noisy, we mean that if we evaluate \(r_i(x)\) multiple times at the same value of \(x\), we get different results. This may happen when a Monte Carlo simulation is used, for instance, or \(r_i(x)\) involves performing a physical experiment.

If \sphinxstylestrong{the residuals are expensive to evaluate}, then estimating derivatives (which requires \(n\) evaluations of each \(r_i(x)\) for every point of interest \(x\)) may be prohibitively expensive. Derivative-free methods are designed to solve the problem with the fewest number of evaluations of the objective as possible.

\sphinxstylestrong{However, if you have provide (or a solver can estimate) derivatives} of \(r_i(x)\), then it is probably a good idea to use one of the many derivative-based solvers (such as \sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least\_squares.html}{one from the SciPy library}).


\section{Parameter Fitting}
\label{\detokenize{info:parameter-fitting}}
A very common problem in many quantitative disciplines is fitting parameters to observed data. Typically, this means that we have developed a model for some proccess, which takes a vector of (known) inputs \(\mathrm{obs}\in\mathbb{R}^N\) and some model parameters \(x=(x_1, \ldots, x_n)\in\mathbb{R}^n\), and computes a (predicted) quantity of interest \(y\in\mathbb{R}\):
\begin{equation*}
\begin{split}y = \mathrm{model}(\mathrm{obs}, x)\end{split}
\end{equation*}
For this model to be useful, we need to determine a suitable choice for the parameters \(x\), which typically cannot be directly observed. A common way of doing this is to calibrate from observed relationships.

Suppose we have some observations of the input-to-output relationship. That is, we have data
\begin{equation*}
\begin{split}(\mathrm{obs}_1, y_1), \ldots, (\mathrm{obs}_m, y_m)\end{split}
\end{equation*}
Then, we try to find the parameters \(x\) which produce the best possible fit to these observations by minimizing the sum-of-squares of the prediction errors:
\begin{equation*}
\begin{split}\min_{x\in\mathbb{R}^n}  \quad  f(x) := \sum_{i=1}^{m}(y_i - \mathrm{model}(\mathrm{obs}_i, x))^2\end{split}
\end{equation*}
which is in the least-squares form required by DFO-LS.

As described above, DFO-LS is a particularly good choice for parameter fitting when the model has noise (e.g. Monte Carlo simulation) or is expensive to evaluate.


\section{Solving Nonlinear Systems of Equations}
\label{\detokenize{info:solving-nonlinear-systems-of-equations}}
Suppose we wish to solve the system of nonlinear equations: find \(x\in\mathbb{R}^n\) satisfying
\begin{equation*}
\begin{split}r_1(x) &= 0 \\
r_2(x) &= 0 \\
&\vdots \\
r_m(x) &= 0\end{split}
\end{equation*}
Such problems can have no solutions, one solution, or many solutions (possibly infinitely many). Often, but certainly not always, the number of solutions depends on whether there are more equations or unknowns: if \(m<n\) we say the system is underdetermined (and there are often multiple solutions), if \(m=n\) we say the system is square (and there is often only one solution), and if \(m>n\) we say the system is overdetermined (and there are often no solutions).

This is not always true \textendash{} there is no solution to the underdetermined system when \(m=1\) and \(n=2\) and we choose \(r_1(x)=\sin(x_1+x_2)-2\), for example.
Similarly, if we take \(n=1\) and \(r_i(x)=i (x-1)(x-2)\), we can make \(m\) as large as we like while keeping \(x=1\) and \(x=2\) as solutions (to the overdetermined system).

If no solution exists, it makes sense to instead search for an \(x\) which approximately satisfies each equation. A common way to do this is to minimize the sum-of-squares of the left-hand-sides:
\begin{equation*}
\begin{split}\min_{x\in\mathbb{R}^n}  \quad  f(x) := \sum_{i=1}^{m}r_i(x)^2\end{split}
\end{equation*}
which is the form required by DFO-LS.

If a solution does exist, then this formulation will also find this (where we will get \(f=0\) at the solution).

\sphinxstylestrong{Which solution?} DFO-LS, and most similar software, will only find one solution to a set of nonlinear equations. Which one it finds is very difficult to predict, and depends very strongly on the point where the solver is started from. Often it finds the closest solution, but there are no guarantees this will be the case. If you need to find all/multiple solutions for your problem, consider techniques such as \sphinxhref{http://www.sciencedirect.com/science/article/pii/0022247X83900550}{deflation}.


\section{Details of the DFO-LS Algorithm}
\label{\detokenize{info:details-of-the-dfo-ls-algorithm}}
DFO-LS is a type of \sphinxstyleemphasis{trust-region} method, a common category of optimization algorithms for nonconvex problems. Given a current estimate of the solution \(x_k\), we compute a model which approximates the objective \(m_k(s)\approx f(x_k+s)\) (for small steps \(s\)), and maintain a value \(\Delta_k>0\) (called the \sphinxstyleemphasis{trust region radius}) which measures the size of \(s\) for which the approximation is good.

At each step, we compute a trial step \(s_k\) designed to make our approximation \(m_k(s)\) small (this task is called the \sphinxstyleemphasis{trust region subproblem}). We evaluate the objective at this new point, and if this provided a good decrease in the objective, we take the step (\(x_{k+1}=x_k+s_k\)), otherwise we stay put (\(x_{k+1}=x_k\)). Based on this information, we choose a new value \(\Delta_{k+1}\), and repeat the process.

In DFO-LS, we construct our approximation \(m_k(s)\) by interpolating a linear approximation for each residual \(r_i(x)\) at several points close to \(x_k\). To make sure our interpolated model is accurate, we need to regularly check that the points are well-spaced, and move them if they aren’t (i.e. improve the geometry of our interpolation points).

A complete description of the DFO-LS algorithm is given in our paper \phantomsection\label{\detokenize{info:id1}}{\hyperref[\detokenize{userguide:cfmr2018}]{\sphinxcrossref{{[}CFMR2018{]}}}}.


\section{References}
\label{\detokenize{info:references}}

\chapter{Using DFO-LS}
\label{\detokenize{userguide:using-dfo-ls}}\label{\detokenize{userguide::doc}}
This section describes the main interface to DFO-LS and how to use it.


\section{Nonlinear Least-Squares Minimization}
\label{\detokenize{userguide:nonlinear-least-squares-minimization}}
DFO-LS is designed to solve the local optimization problem
\begin{equation*}
\begin{split}\min_{x\in\mathbb{R}^n}  &\quad  f(x) := \sum_{i=1}^{m}r_{i}(x)^2 \\
\text{s.t.} &\quad  a \leq x \leq b\end{split}
\end{equation*}
where the bound constraints \(a \leq x \leq b\) are optional.

DFO-LS iteratively constructs an interpolation-based model for the objective, and determines a step using a trust-region framework.
For an in-depth technical description of the algorithm see the paper \phantomsection\label{\detokenize{userguide:id1}}{\hyperref[\detokenize{userguide:cfmr2018}]{\sphinxcrossref{{[}CFMR2018{]}}}}.


\section{How to use DFO-LS}
\label{\detokenize{userguide:how-to-use-dfo-ls}}
The main interface to DFO-LS is via the function \sphinxcode{solve}
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{objfun}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

The input \sphinxcode{objfun} is a Python function which takes an input \(x\in\mathbb{R}^n\) and returns the vector of residuals \([r_1(x)\: \cdots \: r_m(x)]\in\mathbb{R}^m\). Both the input and output of \sphinxcode{objfun} must be one-dimensional NumPy arrays (i.e. with \sphinxcode{x.shape == (n,)} and \sphinxcode{objfun(x).shape == (m,)}).

The input \sphinxcode{x0} is the starting point for the solver, and (where possible) should be set to be the best available estimate of the true solution \(x_{min}\in\mathbb{R}^n\). It should be specified as a one-dimensional NumPy array (i.e. with \sphinxcode{x0.shape == (n,)}).
As DFO-LS is a local solver, providing different values for \sphinxcode{x0} may cause it to return different solutions, with possibly different objective values.

The output of \sphinxcode{dfols.solve} is an object containing:
\begin{itemize}
\item {} 
\sphinxcode{soln.x} - an estimate of the solution, \(x_{min}\in\mathbb{R}^n\), a one-dimensional NumPy array.

\item {} 
\sphinxcode{soln.resid} - the vector of residuals at the calculated solution, \([r_1(x_{min})\:\cdots\: r_m(x_{min})]\), a one-dimensional NumPy array.

\item {} 
\sphinxcode{soln.f} - the objective value at the calculated solution, \(f(x_{min})\), a Float.

\item {} 
\sphinxcode{soln.jacobian} - an estimate of the Jacobian matrix of first derivatives of the residuals, \(J_{i,j} \approx \partial r_i(x_{min})/\partial x_j\), a NumPy array of size \(m\times n\).

\item {} 
\sphinxcode{soln.nf} - the number of evaluations of \sphinxcode{objfun} that the algorithm needed, an Integer.

\item {} 
\sphinxcode{soln.nx} - the number of points \(x\) at which \sphinxcode{objfun} was evaluated, an Integer. This may be different to \sphinxcode{soln.nf} if sample averaging is used.

\item {} 
\sphinxcode{soln.nruns} - the number of runs performed by DFO-LS (more than 1 if using multiple restarts), an Integer.

\item {} 
\sphinxcode{soln.flag} - an exit flag, which can take one of several values (listed below), an Integer.

\item {} 
\sphinxcode{soln.msg} - a description of why the algorithm finished, a String.

\item {} 
\sphinxcode{soln.diagnostic\_info} - a table of diagnostic information showing the progress of the solver, a Pandas DataFrame.

\end{itemize}

The possible values of \sphinxcode{soln.flag} are defined by the following variables:
\begin{itemize}
\item {} 
\sphinxcode{soln.EXIT\_SUCCESS} - DFO-LS terminated successfully (the objective value or trust region radius are sufficiently small).

\item {} 
\sphinxcode{soln.EXIT\_MAXFUN\_WARNING} - maximum allowed objective evaluations reached. This is the most likely return value when using multiple restarts.

\item {} 
\sphinxcode{soln.EXIT\_SLOW\_WARNING} - maximum number of slow iterations reached.

\item {} 
\sphinxcode{soln.EXIT\_FALSE\_SUCCESS\_WARNING} - DFO-LS reached the maximum number of restarts which decreased the objective, but to a worse value than was found in a previous run.

\item {} 
\sphinxcode{soln.EXIT\_INPUT\_ERROR} - error in the inputs.

\item {} 
\sphinxcode{soln.EXIT\_TR\_INCREASE\_ERROR} - error occurred when solving the trust region subproblem.

\item {} 
\sphinxcode{soln.EXIT\_LINALG\_ERROR} - linear algebra error, e.g. the interpolation points produced a singular linear system.

\end{itemize}

These variables are defined in the \sphinxcode{soln} object, so can be accessed with, for example
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{n}{soln}\PYG{o}{.}\PYG{n}{flag} \PYG{o}{==} \PYG{n}{soln}\PYG{o}{.}\PYG{n}{EXIT\PYGZus{}SUCCESS}\PYG{p}{:}
    \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Success!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}


\section{Optional Arguments}
\label{\detokenize{userguide:optional-arguments}}
The \sphinxcode{solve} function has several optional arguments which the user may provide:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{objfun}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{,} \PYG{n}{bounds}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{n}{npt}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{n}{rhobeg}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,}
            \PYG{n}{rhoend}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}8}\PYG{p}{,} \PYG{n}{maxfun}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{n}{nsamples}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,}
            \PYG{n}{user\PYGZus{}params}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{n}{objfun\PYGZus{}has\PYGZus{}noise}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{,}
            \PYG{n}{scaling\PYGZus{}within\PYGZus{}bounds}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

These arguments are:
\begin{itemize}
\item {} 
\sphinxcode{bounds} - a tuple \sphinxcode{(lower, upper)} with the vectors \(a\) and \(b\) of lower and upper bounds on \(x\) (default is \(a_i=-10^{20}\) and \(b_i=10^{20}\)). To set bounds for either \sphinxcode{lower} or \sphinxcode{upper}, but not both, pass a tuple \sphinxcode{(lower, None)} or \sphinxcode{(None, upper)}.

\item {} 
\sphinxcode{npt} - the number of interpolation points to use (default is \sphinxcode{len(x0)+1}). If using restarts, this is the number of points to use in the first run of the solver, before any restarts (and may be optionally increased via settings in \sphinxcode{user\_params}).

\item {} 
\sphinxcode{rhobeg} - the initial value of the trust region radius (default is \(0.1\max(\|x_0\|_{\infty}, 1)\)).

\item {} 
\sphinxcode{rhoend} - minimum allowed value of trust region radius, which determines when a successful termination occurs (default is \(10^{-8}\)).

\item {} 
\sphinxcode{maxfun} - the maximum number of objective evaluations the algorithm may request (default is \(\min(100(n+1),1000)\)).

\item {} 
\sphinxcode{nsamples} - a Python function \sphinxcode{nsamples(delta, rho, iter, nrestarts)} which returns the number of times to evaluate \sphinxcode{objfun} at a given point. This is only applicable for objectives with stochastic noise, when averaging multiple evaluations at the same point produces a more accurate value. The input parameters are the trust region radius (\sphinxcode{delta}), the lower bound on the trust region radius (\sphinxcode{rho}), how many iterations the algorithm has been running for (\sphinxcode{iter}), and how many restarts have been performed (\sphinxcode{nrestarts}). Default is no averaging (i.e. \sphinxcode{nsamples(delta, rho, iter, nrestarts)=1}).

\item {} 
\sphinxcode{user\_params} - a Python dictionary \sphinxcode{\{'param1': val1, 'param2':val2, ...\}} of optional parameters. A full list of available options is given in the next section {\hyperref[\detokenize{advanced::doc}]{\sphinxcrossref{\DUrole{doc}{Advanced Usage}}}}.

\item {} 
\sphinxcode{objfun\_has\_noise} - a flag to indicate whether or not \sphinxcode{objfun} has stochastic noise; i.e. will calling \sphinxcode{objfun(x)} multiple times at the same value of \sphinxcode{x} give different results? This is used to set some sensible default parameters (including using multiple restarts), all of which can be overridden by the values provided in \sphinxcode{user\_params}.

\item {} 
\sphinxcode{scaling\_within\_bounds} - a flag to indicate whether the algorithm should internally shift and scale the entries of \sphinxcode{x} so that the bounds become \(0 \leq x \leq 1\). This is useful is you are setting \sphinxcode{bounds} and the bounds have different orders of magnitude. If \sphinxcode{scaling\_within\_bounds=True}, the values of \sphinxcode{rhobeg} and \sphinxcode{rhoend} apply to the \sphinxstyleemphasis{shifted} variables.

\end{itemize}

In general when using optimization software, it is good practice to scale your variables so that moving each by a given amount has approximately the same impact on the objective function.
The \sphinxcode{scaling\_within\_bounds} flag is designed to provide an easy way to achieve this, if you have set the bounds \sphinxcode{lower} and \sphinxcode{upper}.


\section{A Simple Example}
\label{\detokenize{userguide:a-simple-example}}
Suppose we wish to minimize the \sphinxhref{https://en.wikipedia.org/wiki/Rosenbrock\_function}{Rosenbrock test function}:
\begin{equation*}
\begin{split}\min_{(x_1,x_2)\in\mathbb{R}^2}  &\quad  100(x_2-x_1^2)^2 + (1-x_1)^2 \\\end{split}
\end{equation*}
This function has exactly one local minimum \(f(x_{min})=0\) at \(x_{min}=(1,1)\). We can write this as a least-squares problem as:
\begin{equation*}
\begin{split}\min_{(x_1,x_2)\in\mathbb{R}^2}  &\quad  [10(x_2-x_1^2)]^2 + [1-x_1]^2 \\\end{split}
\end{equation*}
A commonly-used starting point for testing purposes is \(x_0=(-1.2,1)\). The following script shows how to solve this problem using DFO-LS:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} DFO\PYGZhy{}LS example: minimize the Rosenbrock function}
\PYG{k+kn}{from} \PYG{n+nn}{\PYGZus{}\PYGZus{}future\PYGZus{}\PYGZus{}} \PYG{k+kn}{import} \PYG{n}{print\PYGZus{}function}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{dfols}

\PYG{c+c1}{\PYGZsh{} Define the objective function}
\PYG{k}{def} \PYG{n+nf}{rosenbrock}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{10.0} \PYG{o}{*} \PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{1.0} \PYG{o}{\PYGZhy{}} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define the starting point}
\PYG{n}{x0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set random seed (for reproducibility)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Call DFO\PYGZhy{}LS}
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{rosenbrock}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display output}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{soln}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

Note that DFO-LS is a randomized algorithm: in its first phase, it builds an internal approximation to the objective function by sampling it along random directions. In the code above, we set NumPy’s random seed for reproducibility over multiple runs, but this is not required. The output of this script, showing that DFO-LS finds the correct solution, is
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
****** DFO\PYGZhy{}LS Results ******
Solution xmin = [ 1.  1.]
Residual vector = [ \PYGZhy{}2.22044605e\PYGZhy{}15   0.00000000e+00]
Objective value f(xmin) = 4.930380658e\PYGZhy{}30
Needed 36 objective evaluations (at 36 points)
Approximate Jacobian = [[ \PYGZhy{}1.98957443e+01   1.00000000e+01]
 [ \PYGZhy{}1.00000000e+00   8.37285083e\PYGZhy{}16]]
Exit flag = 0
Success: Objective is sufficiently small
****************************
\end{sphinxVerbatim}
\end{quote}

This and all following problems can be found in the \sphinxhref{https://github.com/numericalalgorithmsgroup/dfols/examples}{examples} directory on the DFO-LS Github page.


\section{Adding Bounds and More Output}
\label{\detokenize{userguide:adding-bounds-and-more-output}}
We can extend the above script to add constraints. To do this, we can add the lines
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define bound constraints (lower \PYGZlt{}= x \PYGZlt{}= upper)}
\PYG{n}{lower} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{10.0}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{10.0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{upper} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{l+m+mf}{0.85}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Call DFO\PYGZhy{}LS (with bounds)}
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{rosenbrock}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{,} \PYG{n}{bounds}\PYG{o}{=}\PYG{p}{(}\PYG{n}{lower}\PYG{p}{,} \PYG{n}{upper}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

DFO-LS correctly finds the solution to the constrained problem:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
****** DFO\PYGZhy{}LS Results ******
Solution xmin = [ 0.9   0.81]
Residual vector = [ 0.   0.1]
Objective value f(xmin) = 0.01
Needed 64 objective evaluations (at 64 points)
Approximate Jacobian = [[ \PYGZhy{}1.79999998e+01   9.99999990e+00]
 [ \PYGZhy{}1.00000000e+00  \PYGZhy{}1.26970349e\PYGZhy{}09]]
Exit flag = 0
Success: rho has reached rhoend
****************************
\end{sphinxVerbatim}
\end{quote}

However, we also get a warning that our starting point was outside of the bounds:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
RuntimeWarning: x0 above upper bound, adjusting
\end{sphinxVerbatim}
\end{quote}

DFO-LS automatically fixes this, and moves \(x_0\) to a point within the bounds, in this case \(x_0=(-1.2,0.85)\).

We can also get DFO-LS to print out more detailed information about its progress using the \sphinxhref{https://docs.python.org/3/library/logging.html}{logging} module. To do this, we need to add the following lines:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{logging}
\PYG{n}{logging}\PYG{o}{.}\PYG{n}{basicConfig}\PYG{p}{(}\PYG{n}{level}\PYG{o}{=}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{INFO}\PYG{p}{,} \PYG{n}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}(message)s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} ... (call dfols.solve)}
\end{sphinxVerbatim}
\end{quote}

And we can now see each evaluation of \sphinxcode{objfun}:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Function eval 1 at point 1 has f = 39.65 at x = [\PYGZhy{}1.2   0.85]
Initialising (random directions)
Function eval 2 at point 2 has f = 14.337296 at x = [\PYGZhy{}1.08  0.85]
Function eval 3 at point 3 has f = 55.25 at x = [\PYGZhy{}1.2   0.73]
...
Function eval 63 at point 63 has f = 0.0100000029949496 at x = [ 0.89999999  0.81      ]
Function eval 64 at point 64 has f = 0.00999999999999993 at x = [ 0.9   0.81]
Did a total of 1 run(s)
\end{sphinxVerbatim}
\end{quote}

If we wanted to save this output to a file, we could replace the above call to \sphinxcode{logging.basicConfig()} with
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{logging}\PYG{o}{.}\PYG{n}{basicConfig}\PYG{p}{(}\PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{myfile.log}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{INFO}\PYG{p}{,}
                    \PYG{n}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}(message)s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{filemode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}


\section{Example: Noisy Objective Evaluation}
\label{\detokenize{userguide:example-noisy-objective-evaluation}}
As described in {\hyperref[\detokenize{info::doc}]{\sphinxcrossref{\DUrole{doc}{Overview}}}}, derivative-free algorithms such as DFO-LS are particularly useful when \sphinxcode{objfun} has noise. Let’s modify the previous example to include random noise in our objective evaluation, and compare it to a derivative-based solver:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} DFO\PYGZhy{}LS example: minimize the noisy Rosenbrock function}
\PYG{k+kn}{from} \PYG{n+nn}{\PYGZus{}\PYGZus{}future\PYGZus{}\PYGZus{}} \PYG{k+kn}{import} \PYG{n}{print\PYGZus{}function}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{dfols}

\PYG{c+c1}{\PYGZsh{} Define the objective function}
\PYG{k}{def} \PYG{n+nf}{rosenbrock}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{10.0} \PYG{o}{*} \PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{1.0} \PYG{o}{\PYGZhy{}} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Modified objective function: add 1\PYGZpc{} Gaussian noise}
\PYG{k}{def} \PYG{n+nf}{rosenbrock\PYGZus{}noisy}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{rosenbrock}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mf}{1.0} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define the starting point}
\PYG{n}{x0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set random seed (for reproducibility)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Demonstrate noise in function evaluation:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{objfun(x0) = }\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{rosenbrock\PYGZus{}noisy}\PYG{p}{(}\PYG{n}{x0}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Call DFO\PYGZhy{}LS}
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{rosenbrock\PYGZus{}noisy}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display output}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{soln}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compare with a derivative\PYGZhy{}based solver}
\PYG{k+kn}{import} \PYG{n+nn}{scipy.optimize} \PYG{k+kn}{as} \PYG{n+nn}{opt}
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{opt}\PYG{o}{.}\PYG{n}{least\PYGZus{}squares}\PYG{p}{(}\PYG{n}{rosenbrock\PYGZus{}noisy}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{)}

\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{** SciPy results **}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Solution xmin = }\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{soln}\PYG{o}{.}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Objective value f(xmin) = }\PYG{l+s+si}{\PYGZpc{}.10g}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{p}{(}\PYG{l+m+mf}{2.0} \PYG{o}{*} \PYG{n}{soln}\PYG{o}{.}\PYG{n}{cost}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Needed }\PYG{l+s+si}{\PYGZpc{}g}\PYG{l+s+s2}{ objective evaluations}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{soln}\PYG{o}{.}\PYG{n}{nfev}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exit flag = }\PYG{l+s+si}{\PYGZpc{}g}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{soln}\PYG{o}{.}\PYG{n}{status}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{soln}\PYG{o}{.}\PYG{n}{message}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

The output of this is:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Demonstrate noise in function evaluation:
objfun(x0) = [\PYGZhy{}4.4776183   2.20880346]
objfun(x0) = [\PYGZhy{}4.44306447  2.24929965]
objfun(x0) = [\PYGZhy{}4.48217255  2.17849989]
objfun(x0) = [\PYGZhy{}4.44180389  2.19667014]
objfun(x0) = [\PYGZhy{}4.39545837  2.20903317]

****** DFO\PYGZhy{}LS Results ******
Solution xmin = [ 1.  1.]
Residual vector = [  3.51006670e\PYGZhy{}08   2.00158313e\PYGZhy{}10]
Objective value f(xmin) = 1.232096886e\PYGZhy{}15
Needed 46 objective evaluations (at 46 points)
Approximate Jacobian = [[ \PYGZhy{}2.04330578e+01   1.00296466e+01]
 [ \PYGZhy{}9.88260906e\PYGZhy{}01  \PYGZhy{}3.77364910e\PYGZhy{}03]]
Exit flag = 0
Success: Objective is sufficiently small
****************************


** SciPy results **
Solution xmin = [\PYGZhy{}1.2  1. ]
Objective value f(xmin) = 23.96809472
Needed 5 objective evaluations
Exit flag = 3
{}`xtol{}` termination condition is satisfied.
\end{sphinxVerbatim}
\end{quote}

DFO-LS is able to find the solution with only 10 more function evaluations than in the noise-free case. However SciPy’s derivative-based solver, which has no trouble solving the noise-free problem, is unable to make any progress.

As noted above, DFO-LS has an input parameter \sphinxcode{objfun\_has\_noise} to indicate if \sphinxcode{objfun} has noise in it, which it does in this case. Therefore we can call DFO-LS with
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{rosenbrock\PYGZus{}noisy}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{,} \PYG{n}{objfun\PYGZus{}has\PYGZus{}noise}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

Using this setting, we find the correct solution faster:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
****** DFO\PYGZhy{}LS Results ******
Solution xmin = [ 1.  1.]
Residual vector = [ \PYGZhy{}5.80172077e\PYGZhy{}09   2.10781076e\PYGZhy{}09]
Objective value f(xmin) = 3.810283004e\PYGZhy{}17
Needed 29 objective evaluations (at 29 points)
Approximate Jacobian = [[ \PYGZhy{}1.96671666e+01   9.88784341e+00]
 [ \PYGZhy{}1.00451147e+00   1.43596001e\PYGZhy{}04]]
Exit flag = 0
Success: Objective is sufficiently small
****************************
\end{sphinxVerbatim}
\end{quote}


\section{Example: Parameter Estimation/Data Fitting}
\label{\detokenize{userguide:example-parameter-estimation-data-fitting}}
Next, we show a short example of using DFO-LS to solve a parameter estimation problem (taken from \sphinxhref{https://uk.mathworks.com/help/optim/ug/lsqcurvefit.html\#examples}{here}). Given some observations \((t_i,y_i)\), we wish to calibrate parameters \(x=(x_1,x_2)\) in the exponential decay model
\begin{equation*}
\begin{split}y(t) = x_1 \exp(x_2 t)\end{split}
\end{equation*}
The code for this is:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} DFO\PYGZhy{}LS example: data fitting problem}
\PYG{c+c1}{\PYGZsh{} Originally from:}
\PYG{c+c1}{\PYGZsh{} https://uk.mathworks.com/help/optim/ug/lsqcurvefit.html}
\PYG{k+kn}{from} \PYG{n+nn}{\PYGZus{}\PYGZus{}future\PYGZus{}\PYGZus{}} \PYG{k+kn}{import} \PYG{n}{print\PYGZus{}function}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{dfols}

\PYG{c+c1}{\PYGZsh{} Observations}
\PYG{n}{tdata} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{13.8}\PYG{p}{,} \PYG{l+m+mf}{19.8}\PYG{p}{,} \PYG{l+m+mf}{24.1}\PYG{p}{,} \PYG{l+m+mf}{28.2}\PYG{p}{,} \PYG{l+m+mf}{35.2}\PYG{p}{,}
                  \PYG{l+m+mf}{60.3}\PYG{p}{,} \PYG{l+m+mf}{74.6}\PYG{p}{,} \PYG{l+m+mf}{81.3}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ydata} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{455.2}\PYG{p}{,} \PYG{l+m+mf}{428.6}\PYG{p}{,} \PYG{l+m+mf}{124.1}\PYG{p}{,} \PYG{l+m+mf}{67.3}\PYG{p}{,} \PYG{l+m+mf}{43.2}\PYG{p}{,} \PYG{l+m+mf}{28.1}\PYG{p}{,} \PYG{l+m+mf}{13.1}\PYG{p}{,}
                  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.3}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.5}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Model is y(t) = x[0] * exp(x[1] * t)}
\PYG{k}{def} \PYG{n+nf}{prediction\PYGZus{}error}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{ydata} \PYG{o}{\PYGZhy{}} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{*} \PYG{n}{tdata}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define the starting point}
\PYG{n}{x0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{100.0}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set random seed (for reproducibility)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} We expect exponential decay: set upper bound x[1] \PYGZlt{}= 0}
\PYG{n}{upper} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{1e20}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Call DFO\PYGZhy{}LS}
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{prediction\PYGZus{}error}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{,} \PYG{n}{bounds}\PYG{o}{=}\PYG{p}{(}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{n}{upper}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display output}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{soln}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

The output of this is (noting that DFO-LS moves \(x_0\) to be far away enough from the upper bound)
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
RuntimeWarning: x0 too close to upper bound, adjusting
****** DFO\PYGZhy{}LS Results ******
Solution xmin = [  4.98830860e+02  \PYGZhy{}1.01256863e\PYGZhy{}01]
Residual vector = [\PYGZhy{}0.18167084  0.06098401  0.76276294  0.11962349 \PYGZhy{}0.265898   \PYGZhy{}0.59788818
 \PYGZhy{}1.02611899 \PYGZhy{}1.51235371 \PYGZhy{}1.56145452 \PYGZhy{}1.63266662]
Objective value f(xmin) = 9.504886892
Needed 99 objective evaluations (at 99 points)
Approximate Jacobian = [[ \PYGZhy{}9.12901557e\PYGZhy{}01  \PYGZhy{}4.09843510e+02]
 [ \PYGZhy{}8.59085471e\PYGZhy{}01  \PYGZhy{}6.42808522e+02]
 [ \PYGZhy{}2.47253894e\PYGZhy{}01  \PYGZhy{}1.70205399e+03]
 [ \PYGZhy{}1.34675403e\PYGZhy{}01  \PYGZhy{}1.33017159e+03]
 [ \PYGZhy{}8.71359818e\PYGZhy{}02  \PYGZhy{}1.04752827e+03]
 [ \PYGZhy{}5.75305576e\PYGZhy{}02  \PYGZhy{}8.09280563e+02]
 [ \PYGZhy{}2.83185322e\PYGZhy{}02  \PYGZhy{}4.97239478e+02]
 [ \PYGZhy{}2.22993603e\PYGZhy{}03  \PYGZhy{}6.70749492e+01]
 [ \PYGZhy{}5.24135530e\PYGZhy{}04  \PYGZhy{}1.95045149e+01]
 [ \PYGZhy{}2.65977795e\PYGZhy{}04  \PYGZhy{}1.07858009e+01]]
Exit flag = 0
Success: rho has reached rhoend
****************************
\end{sphinxVerbatim}
\end{quote}

This produces a good fit to the observations.

\noindent{\hspace*{\fill}\sphinxincludegraphics[width=0.750\linewidth]{{data_fitting}.png}\hspace*{\fill}}

To generate this plot, run:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot calibrated model vs. observations}
\PYG{n}{ts} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{90.0}\PYG{p}{)}
\PYG{n}{ys} \PYG{o}{=} \PYG{n}{soln}\PYG{o}{.}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{soln}\PYG{o}{.}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{*} \PYG{n}{ts}\PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{matplotlib.pyplot} \PYG{k+kn}{as} \PYG{n+nn}{plt}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} current axes}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ts}\PYG{p}{,} \PYG{n}{ys}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{tdata}\PYG{p}{,} \PYG{n}{ydata}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bo}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y(t)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{upper right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}


\section{Example: Solving a Nonlinear System of Equations}
\label{\detokenize{userguide:example-solving-a-nonlinear-system-of-equations}}
Lastly, we give an example of using DFO-LS to solve a nonlinear system of equations (taken from \sphinxhref{http://support.sas.com/documentation/cdl/en/imlug/66112/HTML/default/viewer.htm\#imlug\_genstatexpls\_sect004.htm}{here}). We wish to solve the following set of equations
\begin{equation*}
\begin{split}x_1 + x_2 - x_1 x_2 + 2 &= 0, \\
x_1 \exp(-x_2) - 1 &= 0.\end{split}
\end{equation*}
The code for this is:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} DFO\PYGZhy{}LS example: Solving a nonlinear system of equations}
\PYG{c+c1}{\PYGZsh{} Originally from:}
\PYG{c+c1}{\PYGZsh{} http://support.sas.com/documentation/cdl/en/imlug/66112/HTML/default/viewer.htm\PYGZsh{}imlug\PYGZus{}genstatexpls\PYGZus{}sect004.htm}

\PYG{k+kn}{from} \PYG{n+nn}{\PYGZus{}\PYGZus{}future\PYGZus{}\PYGZus{}} \PYG{k+kn}{import} \PYG{n}{print\PYGZus{}function}
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{exp}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{dfols}

\PYG{c+c1}{\PYGZsh{} Want to solve:}
\PYG{c+c1}{\PYGZsh{}   x1 + x2 \PYGZhy{} x1*x2 + 2 = 0}
\PYG{c+c1}{\PYGZsh{}   x1 * exp(\PYGZhy{}x2) \PYGZhy{} 1   = 0}
\PYG{k}{def} \PYG{n+nf}{nonlinear\PYGZus{}system}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{+} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{+} \PYG{l+m+mi}{2}\PYG{p}{,}
                     \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*} \PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Warning: if there are multiple solutions, which one}
\PYG{c+c1}{\PYGZsh{}          DFO\PYGZhy{}LS returns will likely depend on x0!}
\PYG{n}{x0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{2.0}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set random seed (for reproducibility)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Call DFO\PYGZhy{}LS}
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{nonlinear\PYGZus{}system}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display output}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{soln}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

The output of this is
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
****** DFO\PYGZhy{}LS Results ******
Solution xmin = [ 0.09777309 \PYGZhy{}2.32510588]
Residual vector = [ \PYGZhy{}3.16191517e\PYGZhy{}13  \PYGZhy{}3.58602037e\PYGZhy{}12]
Objective value f(xmin) = 1.295951917e\PYGZhy{}23
Needed 17 objective evaluations (at 17 points)
Approximate Jacobian = [[  3.32510506   0.9022256 ]
 [ 10.22775528  \PYGZhy{}1.00001417]]
Exit flag = 0
Success: Objective is sufficiently small
****************************
\end{sphinxVerbatim}
\end{quote}

Here, we see that both entries of the residual vector are very small, so both equations have been solved to high accuracy.


\section{References}
\label{\detokenize{userguide:references}}

\chapter{Advanced Usage}
\label{\detokenize{advanced:advanced-usage}}\label{\detokenize{advanced::doc}}
This section describes different optional user parameters available in DFO-LS.

In the last section ({\hyperref[\detokenize{userguide::doc}]{\sphinxcrossref{\DUrole{doc}{Using DFO-LS}}}}), we introduced \sphinxcode{dfols.solve()}, which has the optional input \sphinxcode{user\_params}. This is a Python dictionary of user parameters. We will now go through the settings which can be changed in this way. More details are available in the paper \phantomsection\label{\detokenize{advanced:id1}}{\hyperref[\detokenize{userguide:cfmr2018}]{\sphinxcrossref{{[}CFMR2018{]}}}}.

The default values, used if no override is given, in some cases vary depending on whether \sphinxcode{objfun} has stochastic noise; that is, whether evaluating \sphinxcode{objfun(x)} several times at the same \sphinxcode{x} gives the same result or not. Whether or not this is the case is determined by the \sphinxcode{objfun\_has\_noise} input to \sphinxcode{dfols.solve()} (and not by inspecting \sphinxcode{objfun}, for instance).


\section{General Algorithm Parameters}
\label{\detokenize{advanced:general-algorithm-parameters}}\begin{itemize}
\item {} 
\sphinxcode{general.rounding\_error\_constant} - Internally, all interpolation points are stored with respect to a base point \(x_b\); that is, we store \(\{y_t-x_b\}\), which reduces the risk of roundoff errors. We shift \(x_b\) to \(x_k\) when \(\|s_k\| \leq \text{const}\|x_k-x_b\|\), where ‘const’ is this parameter. Default is 0.1.

\item {} 
\sphinxcode{general.safety\_step\_thresh} - Threshold for when to call the safety step, \(\|s_k\| \leq \gamma_S \rho_k\). Default is \(\gamma_S =0.5\).

\item {} 
\sphinxcode{general.check\_objfun\_for\_overflow} - Whether to cap the value of \(r_i(x)\) when they are large enough that an OverflowError will be encountered when trying to evaluate \(f(x)\). Default is \sphinxcode{True}.

\end{itemize}


\section{Logging and Output}
\label{\detokenize{advanced:logging-and-output}}\begin{itemize}
\item {} 
\sphinxcode{logging.n\_to\_print\_whole\_x\_vector} - If printing all function evaluations to screen/log file, the maximum \sphinxcode{len(x)} for which the full vector \sphinxcode{x} should be printed also. Default is 6.

\item {} 
\sphinxcode{logging.save\_diagnostic\_info} - Flag so save diagnostic information at each iteration. Default is \sphinxcode{False}.

\item {} 
\sphinxcode{logging.save\_poisedness} - If saving diagnostic information, whether to include the \(\Lambda\)-poisedness of \(Y_k\) in the diagnostic information. This is the most computationally expensive piece of diagnostic information. Default is \sphinxcode{True}.

\item {} 
\sphinxcode{logging.save\_xk} - If saving diagnostic information, whether to include the full vector \(x_k\). Default is \sphinxcode{False}.

\item {} 
\sphinxcode{logging.save\_rk} - If saving diagnostic information, whether to include the full vector \([r_1(x_k)\:\cdots\:r_m(x_k)]\). The value \(f(x_k)\) is always included. Default is \sphinxcode{False}.

\end{itemize}


\section{Initialization of Points}
\label{\detokenize{advanced:initialization-of-points}}\begin{itemize}
\item {} 
\sphinxcode{init.random\_initial\_directions} - Build the initial interpolation set using random directions (as opposed to coordinate directions). Default is \sphinxcode{True}.

\item {} 
\sphinxcode{init.random\_directions\_make\_orthogonal} - If building initial interpolation set with random directions, whether or not these should be orthogonalized. Default is \sphinxcode{True}.

\item {} 
\sphinxcode{init.run\_in\_parallel} - If using random directions, whether or not to ask for all \sphinxcode{objfun} to be evaluated at all points without any intermediate processing. Default is \sphinxcode{False}.

\end{itemize}


\section{Trust Region Management}
\label{\detokenize{advanced:trust-region-management}}\begin{itemize}
\item {} 
\sphinxcode{tr\_radius.eta1} - Threshold for unsuccessful trust region iteration, \(\eta_1\). Default is 0.1.

\item {} 
\sphinxcode{tr\_radius.eta2} - Threshold for very successful trust region iteration, \(\eta_2\). Default is 0.7.

\item {} 
\sphinxcode{tr\_radius.gamma\_dec} - Ratio to decrease \(\Delta_k\) in unsuccessful iteration, \(\gamma_{dec}\). Default is 0.5 for smooth problems or 0.98 for noisy problems (i.e. \sphinxcode{objfun\_has\_noise = True}).

\item {} 
\sphinxcode{tr\_radius.gamma\_inc} - Ratio to increase \(\Delta_k\) in very successful iterations, \(\gamma_{inc}\). Default is 2.

\item {} 
\sphinxcode{tr\_radius.gamma\_inc\_overline} - Ratio of \(\|s_k\|\) to increase \(\Delta_k\) by in very successful iterations, \(\overline{\gamma}_{inc}\). Default is 4.

\item {} 
\sphinxcode{tr\_radius.alpha1} - Ratio to decrease \(\rho_k\) by when it is reduced, \(\alpha_1\). Default is 0.1 for smooth problems or 0.9 for noisy problems (i.e. \sphinxcode{objfun\_has\_noise = True}).

\item {} 
\sphinxcode{tr\_radius.alpha2} - Ratio of \(\rho_k\) to decrease \(\Delta_k\) by when \(\rho_k\) is reduced, \(\alpha_2\). Default is 0.5 for smooth problems or 0.95 for noisy problems (i.e. \sphinxcode{objfun\_has\_noise = True}).

\end{itemize}


\section{Termination on Small Objective Value}
\label{\detokenize{advanced:termination-on-small-objective-value}}\begin{itemize}
\item {} 
\sphinxcode{model.abs\_tol} - Tolerance on \(f(x_k)\); quit if \(f(x_k)\) is below this value. Default is \(10^{-12}\).

\item {} 
\sphinxcode{model.rel\_tol} - Relative tolerance on \(f(x_k)\); quit if \(f(x_k)/f(x_0)\) is below this value. Default is \(10^{-20}\).

\end{itemize}


\section{Termination on Slow Progress}
\label{\detokenize{advanced:termination-on-slow-progress}}\begin{itemize}
\item {} 
\sphinxcode{slow.history\_for\_slow} - History used to determine whether the current iteration is ‘slow’. Default is 5.

\item {} 
\sphinxcode{slow.thresh\_for\_slow} - Threshold for objective decrease used to determine whether the current iteration is ‘slow’. Default is \(10^{-4}\).

\item {} 
\sphinxcode{slow.max\_slow\_iters} - Number of consecutive slow successful iterations before termination (or restart). Default is \sphinxcode{20*len(x0)}.

\end{itemize}


\section{Stochastic Noise Information}
\label{\detokenize{advanced:stochastic-noise-information}}\begin{itemize}
\item {} 
\sphinxcode{noise.quit\_on\_noise\_level} - Flag to quit (or restart) if all \(f(y_t)\) are within noise level of \(f(x_k)\). Default is \sphinxcode{False} for smooth problems or \sphinxcode{True} for noisy problems.

\item {} 
\sphinxcode{noise.scale\_factor\_for\_quit} - Factor of noise level to use in termination criterion. Default is 1.

\item {} 
\sphinxcode{noise.multiplicative\_noise\_level} - Multiplicative noise level in \(f\). Can only specify one of multiplicative or additive noise levels. Default is \sphinxcode{None}.

\item {} 
\sphinxcode{noise.additive\_noise\_level} - Additive noise level in \(f\). Can only specify one of multiplicative or additive noise levels. Default is \sphinxcode{None}.

\end{itemize}


\section{Interpolation Management}
\label{\detokenize{advanced:interpolation-management}}\begin{itemize}
\item {} 
\sphinxcode{interpolation.precondition} - whether or not to scale the interpolation linear system to improve conditioning. Default is \sphinxcode{True}.

\end{itemize}


\section{Regression Model Management}
\label{\detokenize{advanced:regression-model-management}}\begin{itemize}
\item {} 
\sphinxcode{regression.num\_extra\_steps} - In successful iterations, the number of extra points (other than accepting the trust region step) to move, useful when \(|Y_k|>n+1\) (\(n\) is \sphinxcode{len(x0)}). Default is 0.

\item {} 
\sphinxcode{regression.increase\_num\_extra\_steps\_with\_restart} - The amount to increase \sphinxcode{regression.num\_extra\_steps} by with each restarts, for instance if increasing the number of points with each restart. Default is 0.

\item {} 
\sphinxcode{regression.momentum\_extra\_steps} - If moving extra points in successful iterations, whether to use the ‘momentum’ method. If not, uses geometry-improving steps. Default is \sphinxcode{False}.

\end{itemize}


\section{Multiple Restarts}
\label{\detokenize{advanced:multiple-restarts}}\begin{itemize}
\item {} 
\sphinxcode{restarts.use\_restarts} - Whether to do restarts when \(\rho_k\) reaches \(\rho_{end}\), or (optionally) when all points are within noise level of \(f(x_k)\). Default is \sphinxcode{False} for smooth problems or \sphinxcode{True} for noisy problems.

\item {} 
\sphinxcode{restarts.max\_unsuccessful\_restarts} - Maximum number of consecutive unsuccessful restarts allowed (i.e.\textasciitilde{}restarts which did not reduce the objective further). Default is 10.

\item {} 
\sphinxcode{restarts.rhoend\_scale} - Factor to reduce \(\rho_{end}\) by with each restart. Default is 1.

\item {} 
\sphinxcode{restarts.use\_soft\_restarts} - Whether to use soft or hard restarts. Default is \sphinxcode{True}.

\item {} 
\sphinxcode{restarts.soft.num\_geom\_steps} - For soft restarts, the number of points to move. Default is 3.

\item {} 
\sphinxcode{restarts.soft.move\_xk} - For soft restarts, whether to preserve \(x_k\), or move it to the best new point evaluated. Default is \sphinxcode{True}.

\item {} 
\sphinxcode{restarts.increase\_npt} - Whether to increase \(|Y_k|\) with each restart. Default is \sphinxcode{False}.

\item {} 
\sphinxcode{restarts.increase\_npt\_amt} - Amount to increase \(|Y_k|\) by with each restart. Default is 1.

\item {} 
\sphinxcode{restarts.hard.increase\_ndirs\_initial\_amt} - Amount to increase \sphinxcode{growing.ndirs\_initial} by with each hard restart. To avoid a growing phase, it is best to set it to the same value as \sphinxcode{restarts.increase\_npt\_amt}. Default is 1.

\item {} 
\sphinxcode{restarts.hard.use\_old\_rk} - If using hard restarts, whether or not to recycle the objective value at the best iterate found when performing a restart. This saves one objective evaluation. Default is \sphinxcode{True}.

\item {} 
\sphinxcode{restarts.max\_npt} - Maximum allowed value of \(|Y_k|\), useful if increasing with each restart. Default is \sphinxcode{npt}, the input parameter to \sphinxcode{dfols.solve()}.

\item {} 
\sphinxcode{restarts.auto\_detect} - Whether or not to automatically determine when to restart. This is an extra condition, and restarts can still be triggered by small trust region radius, etc. Default is \sphinxcode{True}.

\item {} 
\sphinxcode{restarts.auto\_detect.history} - How many iterations of data on model changes and trust region radii to store. There are two criteria used: trust region radius decreases (no increases over the history, more decreases than no changes), and change in model Jacobian (consistently increasing trend as measured by slope and correlation coefficient of line of best fit). Default is 30.

\item {} 
\sphinxcode{restarts.auto\_detect.min\_chgJ\_slope} - Minimum rate of increase of \(\log(\|J_k-J_{k-1}\|_F)\) over the past iterations to cause a restart. Default is 0.015.

\item {} 
\sphinxcode{restarts.auto\_detect.min\_correl} - Minimum correlation of the data set \((k, \log(\|J_k-J_{k-1}\|_F))\) required to cause a restart. Default is 0.1.

\end{itemize}


\section{Dynamically Growing Initial Set}
\label{\detokenize{advanced:dynamically-growing-initial-set}}\begin{itemize}
\item {} 
\sphinxcode{growing.ndirs\_initial} - Number of initial points to add (excluding \(x_k\)). Default is \sphinxcode{npt-1}.

\item {} 
\sphinxcode{growing.num\_new\_dirns\_each\_iter} - Number of new search directions to add with each iteration where we do not have a full set of search directions. Default is 1.

\item {} 
\sphinxcode{growing.delta\_scale\_new\_dirns} - When adding new search directions, the length of the step as a multiple of \(\Delta_k\). Default is 1, but if setting \sphinxcode{growing.perturb\_trust\_region\_step=True} should be made smaller (e.g. 0.1).

\item {} 
\sphinxcode{growing.do\_geom\_steps} - While still growing the initial set, whether to do geometry-improving steps in the trust region algorithm, as per the usual algorithm. Default is \sphinxcode{False}.

\item {} 
\sphinxcode{growing.safety.do\_safety\_step} - While still growing the initial set, whether to perform safety steps, or the regular trust region steps. Default is \sphinxcode{True}.

\item {} 
\sphinxcode{growing.safety.reduce\_delta} - While still growing the initial set, whether to reduce \(\Delta_k\) in safety steps. Default is \sphinxcode{False}.

\item {} 
\sphinxcode{growing.safety.full\_geom\_step} - While still growing the initial set, whether to do a full geometry-improving step within safety steps (the same as the post-growing phase of the algorithm). Since this involves reducing \(\Delta_k\), cannot be \sphinxcode{True} if \sphinxcode{growing.safety.reduce\_delta} is \sphinxcode{True}. Default is \sphinxcode{False}.

\item {} 
\sphinxcode{growing.full\_rank.use\_full\_rank\_interp} - Whether to perturb the interpolated \(J_k\) to make it full rank, allowing the trust region step to include components in the full search space. If \sphinxcode{True}, setting \sphinxcode{growing.num\_new\_dirns\_each\_iter} to 0 is recommended. Default is \sphinxcode{False}.

\item {} 
\sphinxcode{growing.full\_rank.scale\_factor} - Magnitude of extra components added to \(J_k\). Default is \(10^{-2}\).

\item {} 
\sphinxcode{growing.full\_rank.svd\_scale\_factor} - Floor singular values of \(J_k\) at this factor of the last nonzero value. Default is 1.

\item {} 
\sphinxcode{growing.full\_rank.min\_sing\_val} - Absolute floor on singular values of \(J_k\). Default is \(10^{-6}\).

\item {} 
\sphinxcode{growing.full\_rank.svd\_max\_jac\_cond} - Cap on condition number of \(J_k\) after applying floors to singular values (effectively another floor on the smallest singular value, since the largest singular value is fixed). Default is \(10^8\).

\item {} 
\sphinxcode{growing.reset\_delta} - Whether or not to reset trust region radius \(\Delta_k\) to its initial value at the end of the growing phase. Default is \sphinxcode{False}.

\item {} 
\sphinxcode{growing.reset\_rho} - Whether or not to reset trust region radius lower bound \(\rho_k\) to its initial value at the end of the growing phase. Default is \sphinxcode{False}.

\item {} 
\sphinxcode{growing.gamma\_dec} - Trust region decrease parameter during the growing phase. Default is \sphinxcode{tr\_radius.gamma\_dec}.

\item {} 
\sphinxcode{growing.perturb\_trust\_region\_step} - Whether to perturb the trust region step by an orthogonal direction not yet searched. This is an alternative to \sphinxcode{growing.full\_rank.use\_full\_rank\_interp}. Default is \sphinxcode{False}.

\end{itemize}


\section{References}
\label{\detokenize{advanced:references}}

\chapter{Diagnostic Information}
\label{\detokenize{diagnostic:diagnostic-information}}\label{\detokenize{diagnostic::doc}}
In {\hyperref[\detokenize{userguide::doc}]{\sphinxcrossref{\DUrole{doc}{Using DFO-LS}}}}, we saw that the output of DFO-LS returns a container which includes diagnostic information about the progress of the algorithm (\sphinxcode{soln.diagnostic\_info}). This object is a \sphinxhref{http://pandas.pydata.org/}{Pandas} DataFrame, with one row per iteration of the algorithm. In this section, we explain the meaning of each type of output (the columns of the DataFrame).

To save this information to a CSV file, use:
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Previously: define objfun and x0}

\PYG{c+c1}{\PYGZsh{} Turn on diagnostic information}
\PYG{n}{user\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{logging.save\PYGZus{}diagnostic\PYGZus{}info}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Call DFO\PYGZhy{}LS}
\PYG{n}{soln} \PYG{o}{=} \PYG{n}{dfols}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{n}{objfun}\PYG{p}{,} \PYG{n}{x0}\PYG{p}{,} \PYG{n}{user\PYGZus{}params}\PYG{o}{=}\PYG{n}{user\PYGZus{}params}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Save diagnostic info to CSV}
\PYG{n}{soln}\PYG{o}{.}\PYG{n}{diagnostic\PYGZus{}info}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{myfile.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}

Depending on exactly how DFO-LS terminates, the last row of results may not be fully populated.


\section{Current Iterate}
\label{\detokenize{diagnostic:current-iterate}}\begin{itemize}
\item {} 
\sphinxcode{xk} - Best point found so far (current iterate). This is only saved if \sphinxcode{user\_params{[}'logging.save\_xk'{]} = True}.

\item {} 
\sphinxcode{rk} - The vector of residuals at the current iterate. This is only saved if \sphinxcode{user\_params{[}'logging.save\_rk'{]} = True}.

\item {} 
\sphinxcode{fk} - The value of \(f\) at the current iterate.

\end{itemize}


\section{Trust Region}
\label{\detokenize{diagnostic:trust-region}}\begin{itemize}
\item {} 
\sphinxcode{rho} - The lower bound on the trust region radius \(\rho_k\).

\item {} 
\sphinxcode{delta} - The trust region radius \(\Delta_k\).

\item {} 
\sphinxcode{norm\_sk} - The norm of the trust region step \(\|s_k\|\).

\end{itemize}


\section{Model Interpolation}
\label{\detokenize{diagnostic:model-interpolation}}\begin{itemize}
\item {} 
\sphinxcode{npt} - The number of interpolation points.

\item {} 
\sphinxcode{interpolation\_error} - The sum of squares of the interpolation errors from the interpolated model.

\item {} 
\sphinxcode{interpolation\_condition\_number} - The condition number of the matrix in the interpolation linear system.

\item {} 
\sphinxcode{interpolation\_change\_J\_norm} - The Frobenius norm of the change in Jacobian at this iteration, \(\|J_k-J_{k-1}\|_F\).

\item {} 
\sphinxcode{interpolation\_total\_residual} - The total residual from the interpolation optimization problem.

\item {} 
\sphinxcode{poisedness} - The smallest value of \(\Lambda\) for which the current interpolation set \(Y_k\) is \(\Lambda\)-poised in the current trust region. This is the most expensive piece of information to compute, and is only computed if \sphinxcode{user\_params{[}'logging.save\_poisedness' = True}.

\item {} 
\sphinxcode{max\_distance\_xk} - The maximum distance from any interpolation point to the current iterate.

\item {} 
\sphinxcode{norm\_gk} - The norm of the model gradient \(\|g_k\|\).

\end{itemize}


\section{Iteration Count}
\label{\detokenize{diagnostic:iteration-count}}\begin{itemize}
\item {} 
\sphinxcode{nruns} - The number of times the algorithm has been restarted.

\item {} 
\sphinxcode{nf} - The number of objective evaluations so far (see \sphinxcode{soln.nf})

\item {} 
\sphinxcode{nx} - The number of points at which the objective has been evaluated so far (see \sphinxcode{soln.nx})

\item {} 
\sphinxcode{nsamples} - The total number of objective evaluations used for all current interpolation points.

\item {} 
\sphinxcode{iter\_this\_run} - The number of iterations since the last restart.

\item {} 
\sphinxcode{iters\_total} - The total number of iterations so far.

\end{itemize}


\section{Algorithm Progress}
\label{\detokenize{diagnostic:algorithm-progress}}\begin{itemize}
\item {} 
\sphinxcode{iter\_type} - A text description of what type of iteration we had (e.g. Successful, Safety, etc.)

\item {} 
\sphinxcode{ratio} - The ratio of actual to predicted objective reduction in the trust region step.

\item {} 
\sphinxcode{slow\_iter} - Equal to 1 if the current iteration is successful but slow, 0 if is successful but not slow, and -1 if was not successful.

\end{itemize}


\chapter{Acknowledgements}
\label{\detokenize{index:acknowledgements}}
This software was developed under the supervision of \sphinxhref{https://www.maths.ox.ac.uk/people/coralia.cartis}{Coralia Cartis}, and was supported by the EPSRC Centre For Doctoral Training in \sphinxhref{https://www.maths.ox.ac.uk/study-here/postgraduate-study/industrially-focused-mathematical-modelling-epsrc-cdt}{Industrially Focused Mathematical Modelling} (EP/L015803/1) in collaboration with the \sphinxhref{http://www.nag.com/}{Numerical Algorithms Group}.

\begin{sphinxthebibliography}{CFMR2018}
\bibitem[CFMR2018]{\detokenize{CFMR2018}}{\phantomsection\label{\detokenize{info:cfmr2018}} \begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Cartis, J. Fiala, B. Marteau and L. Roberts, Improving the Flexibility and Robustness of Model-Based Derivative-Free Optimization Solvers, technical report, University of Oxford, (2018).

\end{enumerate}
}
\bibitem[CFMR2018]{\detokenize{CFMR2018}}{\phantomsection\label{\detokenize{userguide:cfmr2018}} \begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Cartis, J. Fiala, B. Marteau and L. Roberts, Improving the Flexibility and Robustness of Model-Based Derivative-Free Optimization Solvers, technical report, University of Oxford, (2018).

\end{enumerate}
}
\bibitem[CFMR2018]{\detokenize{CFMR2018}}{\phantomsection\label{\detokenize{advanced:cfmr2018}} \begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Cartis, J. Fiala, B. Marteau and L. Roberts, Improving the Flexibility and Robustness of Model-Based Derivative-Free Optimization Solvers, technical report, University of Oxford, (2018).

\end{enumerate}
}
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}